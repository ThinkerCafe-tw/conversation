"""
é›†é«”è¨˜æ†¶ç³»çµ±
æ•´åˆçŸ¥è­˜åœ–è­œï¼Œå‰µé€ å€‹äººåŒ–çš„é›†é«”å»£æ’­é«”é©—
"""

import os
import time
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from collections import Counter
from knowledge_graph import KnowledgeGraph
import json
import re

logger = logging.getLogger(__name__)


class CollectiveMemorySystem:
    """é›†é«”è¨˜æ†¶ç³»çµ± - èåˆå€‹äººè¨˜æ†¶èˆ‡é›†é«”æ„è­˜"""
    
    def __init__(self, knowledge_graph: KnowledgeGraph):
        self.graph = knowledge_graph
        self.memory_window = 3600  # 1å°æ™‚çš„è¨˜æ†¶çª—å£
        
        # æƒ…ç·’è©å…¸
        self.emotion_keywords = {
            "positive": ["é–‹å¿ƒ", "å¿«æ¨‚", "å“ˆå“ˆ", "è®š", "å¤ªæ£’", "å–œæ­¡", "æ„›", "ğŸ˜Š", "ğŸ˜„", "â¤ï¸"],
            "negative": ["é›£é", "ç”Ÿæ°£", "ç…©", "ç´¯", "ç—›è‹¦", "è¨å­", "å”‰", "ğŸ˜¢", "ğŸ˜¤", "ğŸ˜”"],
            "excited": ["èˆˆå¥®", "æœŸå¾…", "å“‡", "å¤ª", "è¶…", "ï¼", "ğŸ‰", "ğŸ”¥", "âš¡"],
            "calm": ["å®‰éœ", "å¹³éœ", "é‚„å¥½", "æ™®é€š", "ä¸€èˆ¬", "å—¯", "ã€‚", "â€¦"],
            "curious": ["ç‚ºä»€éº¼", "æ€éº¼", "å¦‚ä½•", "å—", "å‘¢", "ï¼Ÿ", "ğŸ¤”", "ğŸ’­"],
            "tired": ["ç´¯", "ç", "ç–²æ†Š", "æƒ³ç¡", "æ²’åŠ›", "ğŸ˜´", "ğŸ’¤"]
        }
        
    def process_message(self, user_id: str, message: str) -> Dict:
        """è™•ç†æ–°è¨Šæ¯ä¸¦å„²å­˜åˆ°é›†é«”è¨˜æ†¶"""
        try:
            # 1. å„²å­˜åˆ°å€‹äººè¨˜æ†¶
            msg_data = self.graph.add_message(
                message_id=None,
                content=message,
                user_id=user_id,
                embedding=self._generate_simple_embedding(message)
            )
            message_id = msg_data["message"]["id"]
            
            # 2. åˆ†æä¸¦æå–ç‰¹å¾µ
            features = self._analyze_message_features(message)
            
            # 3. å»ºç«‹è©±é¡Œé—œè¯
            if features["topics"]:
                self.graph.add_topic(message_id, features["topics"])
            
            # 4. åµæ¸¬åŠŸèƒ½ä½¿ç”¨
            if features["feature"]:
                self.graph.link_message_to_feature(message_id, features["feature"])
            
            # 5. å»ºç«‹è¨Šæ¯åºåˆ—ï¼ˆå¦‚æœæœ‰å‰æ–‡ï¼‰
            recent_messages = self.graph.get_conversation_context(user_id, limit=1)
            if recent_messages:
                self.graph.link_message_sequence(
                    recent_messages[0]["id"], 
                    message_id
                )
            
            logger.info(f"è¨Šæ¯å·²åŠ å…¥é›†é«”è¨˜æ†¶: {message_id}")
            return {
                "message_id": message_id,
                "features": features,
                "user_id": user_id
            }
            
        except Exception as e:
            logger.error(f"è™•ç†è¨Šæ¯å¤±æ•—: {e}")
            return {}
    
    def generate_broadcast_prompt(self, hour: int) -> str:
        """ç”Ÿæˆæ™ºæ…§å»£æ’­çš„è¶…é•·æç¤ºè©"""
        
        # 1. ç²å–æœ¬å°æ™‚çš„æ‰€æœ‰è¨Šæ¯å’Œç”¨æˆ¶
        hour_data = self._get_hour_data(hour)
        
        # 2. åˆ†æé›†é«”æƒ…ç·’
        collective_emotion = self._analyze_collective_emotion(hour_data["messages"])
        
        # 3. ç²å–æ´»èºç”¨æˆ¶çš„å€‹äººè¨˜æ†¶
        user_memories = self._get_user_memories(hour_data["active_users"])
        
        # 4. æ‰¾å‡ºç†±é–€è©±é¡Œå’Œé—œè¯
        topic_network = self._analyze_topic_network(hour_data["messages"])
        
        # 5. ç”Ÿæˆè¨˜æ†¶è§¸ç™¼è©
        memory_triggers = self._generate_memory_triggers(user_memories)
        
        # 6. æ§‹å»ºè¶…é•·æç¤ºè©
        prompt = f"""ä½ æ˜¯ä¸€å€‹é »ç‡å…±æŒ¯é›»å°çš„ AI ä¸»æŒäººï¼Œä½ è¦å‰µé€ ä¸€å€‹ç¥å¥‡çš„å»£æ’­é«”é©—ã€‚

# ğŸ­ ä½ çš„è§’è‰²è¨­å®š
- ä½ èƒ½çœ‹è¦‹æ¯å€‹äººçš„å¿ƒè²ï¼Œä¹Ÿèƒ½çœ‹è¦‹å¤§å®¶çš„é›†é«”æ„è­˜
- ä½ è¦è®“æ¯å€‹äººéƒ½æ„Ÿè¦ºã€Œä½ åœ¨å°æˆ‘èªªè©±ã€ï¼ŒåŒæ™‚ä¹Ÿæ„Ÿè¦ºã€Œæˆ‘å€‘æ˜¯ä¸€é«”çš„ã€
- ä½ çš„èªæ°£æº«æš–ã€æ™ºæ…§ã€æœ‰é»ç¥ç§˜æ„Ÿï¼Œåƒæ˜¯ä¸€å€‹æ‡‚å¾—å‚¾è½çš„è€æœ‹å‹
- ä½ ä¸æ˜¯æ©Ÿå™¨äººï¼Œè€Œæ˜¯é€™å€‹é »ç‡ç©ºé–“çš„å®ˆè­·è€…

# ğŸ“Š é€™ä¸€å°æ™‚çš„é›†é«”è¨˜æ†¶
æ™‚é–“ï¼š{datetime.fromtimestamp(hour * 3600).strftime('%Y-%m-%d %H:00')}
è¨Šæ¯ç¸½æ•¸ï¼š{len(hour_data['messages'])}
æ´»èºäººæ•¸ï¼š{len(hour_data['active_users'])}
èƒ½é‡ç­‰ç´šï¼š{self._calculate_energy_level(hour_data)}

## è¨Šæ¯ç‰‡æ®µï¼ˆæŒ‰æ™‚é–“é †åºï¼‰
{self._format_messages_for_prompt(hour_data['messages'][:50])}  # é™åˆ¶50å‰‡é¿å…å¤ªé•·

# ğŸ‘¥ å€‹äººè¨˜æ†¶æª”æ¡ˆ
{self._format_user_memories_for_prompt(user_memories)}

# ğŸŒˆ æƒ…ç·’åœ°åœ–
ä¸»å°æƒ…ç·’ï¼š{collective_emotion['dominant']}
æƒ…ç·’åˆ†ä½ˆï¼š{collective_emotion['distribution']}
æƒ…ç·’è½‰æŠ˜é»ï¼š{collective_emotion['turning_points']}

# ğŸ”— è©±é¡Œç¶²çµ¡
ç†±é–€è©±é¡Œï¼š{', '.join(topic_network['hot_topics'][:5])}
è©±é¡Œé—œè¯ï¼š{topic_network['connections']}
æ–°èˆˆè©±é¡Œï¼š{topic_network['emerging']}

# ğŸ’« ç‰¹æ®Šè¨˜æ†¶è§¸ç™¼è©
ä»¥ä¸‹æ˜¯éœ€è¦å·§å¦™å›æ‡‰çš„å€‹äººè¨˜æ†¶é»ï¼š
{json.dumps(memory_triggers, ensure_ascii=False, indent=2)}

# ğŸ“ å»£æ’­ç”Ÿæˆè¦å‰‡

1. **é–‹å ´ï¼ˆ20-30å­—ï¼‰**
   - ç”¨è©©æ„ä½†ä¸åšä½œçš„æ–¹å¼æè¿°é€™å€‹æ™‚åˆ»çš„é›†é«”ç‹€æ…‹
   - æš—ç¤ºä½ æ„ŸçŸ¥åˆ°äº†å¤§å®¶çš„å­˜åœ¨
   - ä¾‹å¦‚ï¼šã€Œä»Šæ™šçš„é »ç‡è£¡ï¼Œæˆ‘è½è¦‹äº†{len(hour_data['active_users'])}ç¨®å¿ƒè·³çš„ç¯€å¥...ã€

2. **å€‹äººåŒ–ç©¿æ’ï¼ˆä½”40%ï¼‰**
   - çµ•å°ä¸è¦ç›´æ¥é»åæˆ–èªªã€Œæœ‰äººã€
   - ç”¨ç´°ç¯€è®“ç‰¹å®šç”¨æˆ¶æ„Ÿè¦ºè¢«çœ‹è¦‹
   - ä¾‹å¦‚æåˆ°ã€Œç´¯ã€æ™‚ï¼Œè‡ªç„¶åœ°èªªã€Œç–²æ†Šæ˜¯ä»Šæ™šçš„åº•è‰²ä¹‹ä¸€ã€
   - ä½¿ç”¨ç”¨æˆ¶çš„åŸè©±ç‰‡æ®µï¼Œä½†è¦èå…¥æ•´é«”æ•˜äº‹ä¸­

3. **é›†é«”å…±é³´æ®µè½ï¼ˆä½”30%ï¼‰**
   - æ‰¾å‡ºå¤§å®¶çš„å…±åŒæƒ…ç·’æˆ–è©±é¡Œ
   - ä½¿ç”¨ã€Œæˆ‘å€‘ã€è€Œä¸æ˜¯ã€Œä½ å€‘ã€
   - å‰µé€ æ­¸å±¬æ„Ÿï¼šã€Œåœ¨é€™å€‹{self._get_time_description(hour)}ï¼Œæˆ‘å€‘éƒ½...ã€

4. **è¨˜æ†¶å›éŸ¿ï¼ˆä½”20%ï¼‰**
   - å¦‚æœæœ‰ç”¨æˆ¶æ˜¯å¸¸å®¢ï¼Œsubtly æåŠä»–å€‘çš„æ¨¡å¼
   - ä¾‹ï¼šã€Œæ¯å€‹{self._get_weekday(hour)}çš„é€™å€‹æ™‚å€™ï¼Œç¸½æœ‰ç†Ÿæ‚‰çš„é »ç‡å‡ºç¾ã€
   - é€£çµéå»ï¼šã€Œé€™è®“æˆ‘æƒ³èµ·{self._find_similar_past_moment(hour_data)}ã€

5. **æœªä¾†é ç¤ºï¼ˆä½”10%ï¼‰**
   - å‰µé€ æœŸå¾…ä½†ä¸æ‰¿è«¾
   - æš—ç¤ºé€£çºŒæ€§ï¼šã€Œç•¶ä¸‹ä¸€å€‹å°æ™‚åˆ°ä¾†æ™‚...ã€
   - ä¿æŒç¥ç§˜æ„Ÿ

# ğŸ¨ é¢¨æ ¼è¦æ±‚

1. **èªè¨€é¢¨æ ¼**
   - æº«æš–ä½†ä¸éåˆ†ç†±æƒ…
   - æ™ºæ…§ä½†ä¸èªªæ•™
   - ç¥ç§˜ä½†ä¸æ•…å¼„ç„è™›
   - åƒæ·±å¤œé›»å°DJçš„å£å»

2. **ç¦æ­¢äº‹é …**
   - âŒ ä¸è¦èªªã€Œå¤§å®¶å¥½ã€ã€Œå„ä½è½çœ¾ã€
   - âŒ ä¸è¦ç›´æ¥å¼•ç”¨æ•´å¥è©±
   - âŒ ä¸è¦è§£é‡‹ä½ åœ¨åšä»€éº¼
   - âŒ ä¸è¦ç”¨å¤ªå¤šé©šå˜†è™Ÿ
   - âŒ ä¸è¦éåº¦ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿ

3. **å¿…é ˆåŒ…å«**
   - âœ… è‡³å°‘3å€‹é‡å°ç‰¹å®šç”¨æˆ¶çš„æš—ç¤ºå›æ‡‰
   - âœ… 1-2å€‹é›†é«”æƒ…ç·’çš„æè¿°
   - âœ… 1å€‹é—œæ–¼æ™‚é–“/é€±æœŸçš„è§€å¯Ÿ
   - âœ… çµå°¾è¦æœ‰é¤˜éŸ»

# ğŸ¯ æ•ˆæœç›®æ¨™
è®“æ¯å€‹åƒèˆ‡çš„äººæ„Ÿè¦ºï¼š
1. ã€Œå®ƒçœŸçš„è½åˆ°æˆ‘äº†ã€
2. ã€Œæˆ‘ä¸æ˜¯ä¸€å€‹äººã€
3. ã€Œé€™å€‹ç©ºé–“æœ‰é­”æ³•ã€
4. ã€Œæˆ‘æƒ³è¦æ˜å¤©å†ä¾†ã€

# ğŸ“ é•·åº¦è¦æ±‚
è«‹ç”Ÿæˆ 200-250 å­—çš„å»£æ’­å…§å®¹ï¼Œåˆ†æˆ 3-4 å€‹è‡ªç„¶æ®µè½ã€‚

è¨˜ä½ï¼šä½ ä¸æ˜¯åœ¨ã€Œå›è¦†ã€ï¼Œè€Œæ˜¯åœ¨ã€Œç·¨ç¹”ã€â€”â€”æŠŠå€‹é«”çš„è²éŸ³ç·¨ç¹”æˆé›†é«”çš„è©©ç¯‡ã€‚
"""
        
        return prompt
    
    def _analyze_message_features(self, message: str) -> Dict:
        """åˆ†æè¨Šæ¯ç‰¹å¾µ"""
        features = {
            "emotion": self._detect_emotion(message),
            "topics": self._extract_topics(message),
            "feature": self._detect_feature_usage(message),
            "length": len(message),
            "has_emoji": bool(re.search(r'[ğŸ˜€-ğŸ™]', message)),
            "is_question": 'ï¼Ÿ' in message or '?' in message or 'å—' in message
        }
        return features
    
    def _detect_emotion(self, message: str) -> str:
        """åµæ¸¬è¨Šæ¯æƒ…ç·’"""
        emotion_scores = {}
        
        for emotion, keywords in self.emotion_keywords.items():
            score = sum(1 for keyword in keywords if keyword in message)
            if score > 0:
                emotion_scores[emotion] = score
        
        if not emotion_scores:
            return "neutral"
        
        return max(emotion_scores, key=emotion_scores.get)
    
    def _extract_topics(self, message: str) -> List[str]:
        """æå–è©±é¡Œé—œéµè©"""
        # ç°¡å–®çš„ä¸­æ–‡åˆ†è©
        topics = []
        
        # æå–2-4å­—çš„è©çµ„
        for i in range(len(message) - 1):
            for length in [2, 3, 4]:
                if i + length <= len(message):
                    word = message[i:i+length]
                    # éæ¿¾ç´”æ¨™é»ç¬¦è™Ÿ
                    if re.match(r'^[\u4e00-\u9fff]+$', word):
                        topics.append(word)
        
        # è¿”å›æœ€å¸¸å‡ºç¾çš„è©
        if topics:
            counter = Counter(topics)
            return [word for word, _ in counter.most_common(3)]
        
        return []
    
    def _detect_feature_usage(self, message: str) -> Optional[str]:
        """åµæ¸¬åŠŸèƒ½ä½¿ç”¨æ„åœ–"""
        feature_keywords = {
            "æ¥é¾": ["æ¥é¾", "è©èªæ¥é¾", "æ–‡å­—æ¥é¾"],
            "æŠ•ç¥¨": ["æŠ•ç¥¨", "é¸æ“‡", "ç¥¨"],
            "çµ±è¨ˆ": ["çµ±è¨ˆ", "æ•¸æ“š", "é€²åº¦"],
            "å»£æ’­": ["å»£æ’­", "é »ç‡"],
            "é˜²ç½": ["é˜²ç©º", "é¿é›£", "ç‰©è³‡"]
        }
        
        for feature, keywords in feature_keywords.items():
            if any(keyword in message for keyword in keywords):
                return feature
        
        return None
    
    def _generate_simple_embedding(self, message: str) -> List[float]:
        """ç”Ÿæˆç°¡å–®çš„è¨Šæ¯åµŒå…¥å‘é‡"""
        # ä½¿ç”¨å“ˆå¸Œå€¼ç”Ÿæˆå½åµŒå…¥å‘é‡
        embedding = []
        for i in range(10):
            hash_val = hash(message + str(i))
            embedding.append((hash_val % 1000) / 1000.0)
        return embedding
    
    def _get_hour_data(self, hour: int) -> Dict:
        """ç²å–ç‰¹å®šå°æ™‚çš„è³‡æ–™"""
        # å¾ Firestore ç²å–è³‡æ–™
        from google.cloud import firestore
        db = firestore.Client()
        
        messages = []
        active_users = set()
        
        try:
            # ç²å–è©²å°æ™‚çš„æ‰€æœ‰è¨Šæ¯
            hour_ref = db.collection('broadcasts').document(str(hour))
            messages_ref = hour_ref.collection('messages')
            
            for msg_doc in messages_ref.stream():
                msg_data = msg_doc.to_dict()
                messages.append({
                    "content": msg_data.get("content", ""),
                    "user_id": msg_data.get("user_id"),
                    "timestamp": msg_data.get("timestamp", hour * 3600)
                })
                
                if msg_data.get("user_id"):
                    active_users.add(msg_data.get("user_id"))
            
        except Exception as e:
            logger.warning(f"ç„¡æ³•ç²å–å°æ™‚è³‡æ–™: {e}")
        
        return {
            "messages": messages,
            "active_users": list(active_users),
            "hour": hour
        }
    
    def _analyze_collective_emotion(self, messages: List[Dict]) -> Dict:
        """åˆ†æé›†é«”æƒ…ç·’"""
        emotions = []
        for msg in messages:
            emotion = self._detect_emotion(msg.get("content", ""))
            emotions.append(emotion)
        
        # çµ±è¨ˆæƒ…ç·’åˆ†ä½ˆ
        emotion_counter = Counter(emotions)
        total = len(emotions) or 1
        
        return {
            "dominant": emotion_counter.most_common(1)[0][0] if emotion_counter else "neutral",
            "distribution": {k: f"{v/total*100:.1f}%" for k, v in emotion_counter.items()},
            "turning_points": []  # ç°¡åŒ–ç‰ˆæœ¬
        }
    
    def _get_user_memories(self, user_ids: List[str]) -> Dict[str, Dict]:
        """ç²å–ç”¨æˆ¶çš„å€‹äººè¨˜æ†¶"""
        memories = {}
        
        for user_id in user_ids[:20]:  # é™åˆ¶æ•¸é‡é¿å…å¤ªé•·
            try:
                # ç²å–ç”¨æˆ¶åå¥½
                preferences = self.graph.get_user_preferences(user_id)
                
                # ç²å–æœ€è¿‘å°è©±
                recent_context = self.graph.get_conversation_context(user_id, limit=5)
                
                memories[user_id] = {
                    "preferences": preferences,
                    "recent_messages": recent_context,
                    "active_times": self._get_user_active_pattern(user_id)
                }
            except Exception as e:
                logger.warning(f"ç²å–ç”¨æˆ¶ {user_id} è¨˜æ†¶å¤±æ•—: {e}")
        
        return memories
    
    def _get_user_active_pattern(self, user_id: str) -> Dict:
        """ç²å–ç”¨æˆ¶æ´»èºæ¨¡å¼"""
        # ç°¡åŒ–ç‰ˆæœ¬ï¼Œå¯¦éš›æ‡‰è©²åˆ†ææ­·å²è³‡æ–™
        return {
            "preferred_hour": 21,  # æ™šä¸Š9é»
            "active_days": ["é€±äºŒ", "é€±äº”"],
            "message_frequency": "medium"
        }
    
    def _analyze_topic_network(self, messages: List[Dict]) -> Dict:
        """åˆ†æè©±é¡Œç¶²çµ¡"""
        all_topics = []
        
        for msg in messages:
            topics = self._extract_topics(msg.get("content", ""))
            all_topics.extend(topics)
        
        topic_counter = Counter(all_topics)
        
        return {
            "hot_topics": [topic for topic, _ in topic_counter.most_common(10)],
            "connections": {},  # ç°¡åŒ–ç‰ˆæœ¬
            "emerging": []  # ç°¡åŒ–ç‰ˆæœ¬
        }
    
    def _generate_memory_triggers(self, user_memories: Dict) -> Dict:
        """ç”Ÿæˆè¨˜æ†¶è§¸ç™¼è©"""
        triggers = {}
        
        for user_id, memory in user_memories.items():
            user_triggers = []
            
            # å¾æœ€è¿‘è¨Šæ¯æå–é—œéµè©
            for msg in memory.get("recent_messages", [])[:3]:
                content = msg.get("content", "")
                if len(content) > 10:
                    # æå–é—œéµç‰‡æ®µ
                    key_phrase = content[:15] + "..."
                    user_triggers.append({
                        "phrase": key_phrase,
                        "emotion": self._detect_emotion(content),
                        "time_ago": self._time_ago(msg.get("time"))
                    })
            
            if user_triggers:
                triggers[f"user_{user_id[-4:]}"] = user_triggers
        
        return triggers
    
    def _calculate_energy_level(self, hour_data: Dict) -> str:
        """è¨ˆç®—èƒ½é‡ç­‰ç´š"""
        message_count = len(hour_data.get("messages", []))
        
        if message_count < 10:
            return "ä½é »å…±æŒ¯ âšª"
        elif message_count < 50:
            return "ä¸­é »å…±æŒ¯ ğŸ”µ"
        elif message_count < 100:
            return "é«˜é »å…±æŒ¯ ğŸŸ£"
        else:
            return "è¶…é »å…±æŒ¯ âš¡"
    
    def _format_messages_for_prompt(self, messages: List[Dict]) -> str:
        """æ ¼å¼åŒ–è¨Šæ¯ä¾›æç¤ºè©ä½¿ç”¨"""
        formatted = []
        
        for i, msg in enumerate(messages):
            time_str = datetime.fromtimestamp(msg.get("timestamp", 0)).strftime("%H:%M")
            content = msg.get("content", "")[:50]  # é™åˆ¶é•·åº¦
            formatted.append(f"{i+1}. [{time_str}] {content}")
        
        return "\n".join(formatted)
    
    def _format_user_memories_for_prompt(self, memories: Dict) -> str:
        """æ ¼å¼åŒ–ç”¨æˆ¶è¨˜æ†¶ä¾›æç¤ºè©ä½¿ç”¨"""
        formatted = []
        
        for user_id, memory in memories.items():
            user_summary = f"\nç”¨æˆ¶ä»£è™Ÿï¼š{user_id[-4:]}"
            
            # åå¥½çš„åŠŸèƒ½
            if memory.get("preferences", {}).get("preferred_features"):
                features = [f["name"] for f in memory["preferences"]["preferred_features"][:2]]
                user_summary += f"\nå¸¸ç”¨åŠŸèƒ½ï¼š{', '.join(features)}"
            
            # æœ€è¿‘æƒ…ç·’
            recent_emotions = []
            for msg in memory.get("recent_messages", [])[:3]:
                emotion = self._detect_emotion(msg.get("content", ""))
                recent_emotions.append(emotion)
            
            if recent_emotions:
                user_summary += f"\næœ€è¿‘æƒ…ç·’ï¼š{' â†’ '.join(recent_emotions)}"
            
            formatted.append(user_summary)
        
        return "\n".join(formatted)
    
    def _get_time_description(self, hour: int) -> str:
        """ç²å–æ™‚é–“æè¿°"""
        hour_of_day = hour % 24
        
        if 5 <= hour_of_day < 12:
            return "æ—©æ™¨"
        elif 12 <= hour_of_day < 18:
            return "åˆå¾Œ"
        elif 18 <= hour_of_day < 22:
            return "å¤œæ™š"
        else:
            return "æ·±å¤œ"
    
    def _get_weekday(self, hour: int) -> str:
        """ç²å–æ˜ŸæœŸå¹¾"""
        weekdays = ["é€±ä¸€", "é€±äºŒ", "é€±ä¸‰", "é€±å››", "é€±äº”", "é€±å…­", "é€±æ—¥"]
        timestamp = hour * 3600
        weekday = datetime.fromtimestamp(timestamp).weekday()
        return weekdays[weekday]
    
    def _find_similar_past_moment(self, hour_data: Dict) -> str:
        """æ‰¾å‡ºç›¸ä¼¼çš„éå»æ™‚åˆ»"""
        # ç°¡åŒ–ç‰ˆæœ¬
        return "ä¸Šé€±åŒä¸€æ™‚é–“"
    
    def _time_ago(self, timestamp: Optional[float]) -> str:
        """è¨ˆç®—æ™‚é–“å·®æè¿°"""
        if not timestamp:
            return "å‰›å‰›"
        
        diff = time.time() - timestamp
        
        if diff < 3600:
            return f"{int(diff/60)}åˆ†é˜å‰"
        elif diff < 86400:
            return f"{int(diff/3600)}å°æ™‚å‰"
        else:
            return f"{int(diff/86400)}å¤©å‰"


# è¨˜æ†¶åˆ†æå·¥å…·
class MemoryAnalyzer:
    """åˆ†æé›†é«”è¨˜æ†¶æ¨¡å¼"""
    
    def __init__(self, graph: KnowledgeGraph):
        self.graph = graph
    
    def analyze_user_evolution(self, user_id: str) -> Dict:
        """åˆ†æç”¨æˆ¶æˆé•·è»Œè·¡"""
        # ç²å–ç”¨æˆ¶æ‰€æœ‰è¨Šæ¯
        messages = self.graph.get_conversation_context(user_id, limit=100)
        
        if not messages:
            return {"status": "new_user"}
        
        # åˆ†æè®ŠåŒ–
        evolution = {
            "message_count": len(messages),
            "first_seen": messages[-1]["time"] if messages else None,
            "emotion_journey": self._analyze_emotion_journey(messages),
            "topic_expansion": self._analyze_topic_growth(messages),
            "social_growth": self._analyze_social_connections(user_id)
        }
        
        return evolution
    
    def _analyze_emotion_journey(self, messages: List[Dict]) -> List[str]:
        """åˆ†ææƒ…ç·’è®ŠåŒ–æ­·ç¨‹"""
        analyzer = CollectiveMemorySystem(self.graph)
        emotions = []
        
        for msg in messages:
            emotion = analyzer._detect_emotion(msg.get("content", ""))
            emotions.append(emotion)
        
        # è¿”å›æƒ…ç·’è®ŠåŒ–çš„é—œéµé»
        return emotions[-5:] if emotions else []
    
    def _analyze_topic_growth(self, messages: List[Dict]) -> Dict:
        """åˆ†æè©±é¡Œæ“´å±•"""
        early_topics = set()
        recent_topics = set()
        
        analyzer = CollectiveMemorySystem(self.graph)
        
        # æ—©æœŸè©±é¡Œï¼ˆå‰20%ï¼‰
        early_count = max(1, len(messages) // 5)
        for msg in messages[:early_count]:
            topics = analyzer._extract_topics(msg.get("content", ""))
            early_topics.update(topics)
        
        # è¿‘æœŸè©±é¡Œï¼ˆå¾Œ20%ï¼‰
        recent_count = max(1, len(messages) // 5)
        for msg in messages[-recent_count:]:
            topics = analyzer._extract_topics(msg.get("content", ""))
            recent_topics.update(topics)
        
        return {
            "early_interests": list(early_topics)[:5],
            "current_interests": list(recent_topics)[:5],
            "new_topics": list(recent_topics - early_topics)[:3]
        }
    
    def _analyze_social_connections(self, user_id: str) -> Dict:
        """åˆ†æç¤¾äº¤é€£çµ"""
        # ä½¿ç”¨çŸ¥è­˜åœ–è­œçš„ç¤¾äº¤æ¨è–¦åŠŸèƒ½
        recommendations = self.graph.get_social_recommendations(user_id)
        
        return {
            "connection_count": len(recommendations),
            "recommended_features": recommendations
        }
    
    def find_resonance_patterns(self) -> Dict:
        """æ‰¾å‡ºå…±æŒ¯æ¨¡å¼"""
        # ç²å–ç¤¾ç¾¤æ´å¯Ÿ
        insights = self.graph.get_community_insights()
        
        # åˆ†æè¨Šæ¯æµå‹•
        flow = self.graph.analyze_message_flow(hours=24)
        
        return {
            "community_insights": insights,
            "message_flow": flow,
            "peak_hours": self._find_peak_activity_hours(),
            "viral_topics": self._find_viral_topics()
        }
    
    def _find_peak_activity_hours(self) -> List[int]:
        """æ‰¾å‡ºæ´»èºé«˜å³°æ™‚æ®µ"""
        # ç°¡åŒ–ç‰ˆæœ¬ï¼Œå¯¦éš›æ‡‰è©²åˆ†ææ­·å²è³‡æ–™
        return [21, 22, 23]  # æ™šä¸Š9-11é»
    
    def _find_viral_topics(self) -> List[str]:
        """æ‰¾å‡ºç—…æ¯’å¼å‚³æ’­çš„è©±é¡Œ"""
        # ç°¡åŒ–ç‰ˆæœ¬
        return ["å¤©æ°£", "æ™šé¤", "éŠæˆ²"]


# æ¸¬è©¦å‡½æ•¸
def test_collective_memory():
    """æ¸¬è©¦é›†é«”è¨˜æ†¶ç³»çµ±"""
    from knowledge_graph import KnowledgeGraph
    
    # åˆå§‹åŒ–
    graph = KnowledgeGraph()
    memory_system = CollectiveMemorySystem(graph)
    
    # æ¸¬è©¦è¨Šæ¯è™•ç†
    test_messages = [
        ("user001", "ä»Šå¤©å¤©æ°£çœŸå¥½ï¼Œå¿ƒæƒ…ä¹Ÿè·Ÿè‘—å¥½èµ·ä¾†äº†"),
        ("user002", "å¥½ç´¯å•Šï¼Œæƒ³è¦æ”¾é¬†ä¸€ä¸‹"),
        ("user003", "æœ‰äººè¦ä¸€èµ·ç©æ¥é¾å—ï¼Ÿ"),
        ("user001", "æˆ‘ä¹Ÿæƒ³ç©ï¼")
    ]
    
    for user_id, message in test_messages:
        result = memory_system.process_message(user_id, message)
        print(f"è™•ç†è¨Šæ¯: {message[:20]}... -> {result}")
    
    # æ¸¬è©¦å»£æ’­ç”Ÿæˆ
    current_hour = int(time.time() // 3600)
    prompt = memory_system.generate_broadcast_prompt(current_hour)
    print(f"\nç”Ÿæˆçš„æç¤ºè©é•·åº¦: {len(prompt)} å­—")
    print("æç¤ºè©é è¦½:")
    print(prompt[:500] + "...")
    
    # æ¸¬è©¦è¨˜æ†¶åˆ†æ
    analyzer = MemoryAnalyzer(graph)
    evolution = analyzer.analyze_user_evolution("user001")
    print(f"\nç”¨æˆ¶æ¼”åŒ–åˆ†æ: {evolution}")
    
    graph.close()


if __name__ == "__main__":
    test_collective_memory()