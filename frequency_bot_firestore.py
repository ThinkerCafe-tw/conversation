"""
é »ç‡å…±æŒ¯å»£æ’­ç³»çµ± - Firestore ç‰ˆæœ¬
ä½¿ç”¨ Google Firestore å¯¦ç¾é›¶æˆæœ¬æ–¹æ¡ˆ
"""

import os
import time
import logging
from datetime import datetime, timedelta
from google.cloud import firestore
from google.cloud.firestore_v1 import Increment
from google.api_core import retry
import google.generativeai as genai
import sentry_sdk
from knowledge_graph import KnowledgeGraph
from collective_memory import CollectiveMemorySystem, MemoryAnalyzer

logger = logging.getLogger(__name__)


class FrequencyBotFirestore:
    def __init__(self, knowledge_graph=None):
        """åˆå§‹åŒ–é »ç‡å»£æ’­æ©Ÿå™¨äºº (Firestore ç‰ˆæœ¬)"""
        # åˆå§‹åŒ– Firestore
        self.db = firestore.Client()
        
        # Gemini API è¨­å®š
        genai.configure(api_key=os.getenv('GEMINI_API_KEY'))
        self.model = genai.GenerativeModel('gemini-1.5-flash')
        
        # è¨­å®šé›†åˆåç¨±
        self.broadcasts_collection = 'broadcasts'
        self.generated_collection = 'generated_broadcasts'
        
        # åˆå§‹åŒ–çŸ¥è­˜åœ–è­œå’Œé›†é«”è¨˜æ†¶ç³»çµ±
        self.graph = knowledge_graph
        if self.graph:
            self.memory_system = CollectiveMemorySystem(self.graph)
            self.memory_analyzer = MemoryAnalyzer(self.graph)
        else:
            self.memory_system = None
            self.memory_analyzer = None
        
    def add_to_broadcast(self, message: str, user_id: str = None):
        """å°‡è¨Šæ¯åŠ å…¥å»£æ’­æ± ä¸¦åŒæ­¥åˆ°é›†é«”è¨˜æ†¶"""
        current_hour = int(time.time()) // 3600
        
        # ä½¿ç”¨æ‰¹æ¬¡å¯«å…¥æé«˜æ•ˆèƒ½
        batch = self.db.batch()
        
        # å„²å­˜è¨Šæ¯
        message_ref = self.db.collection(self.broadcasts_collection).document(str(current_hour)).collection('messages').document()
        message_data = {
            'content': message,
            'user_id': user_id,
            'timestamp': datetime.now()
        }
        batch.set(message_ref, message_data)
        
        # åŒæ­¥åˆ°é›†é«”è¨˜æ†¶ç³»çµ±
        if self.memory_system and user_id:
            try:
                memory_result = self.memory_system.process_message(user_id, message)
                logger.info(f"è¨Šæ¯å·²åŠ å…¥é›†é«”è¨˜æ†¶: {memory_result.get('message_id')}")
            except Exception as e:
                logger.warning(f"ç„¡æ³•åŠ å…¥é›†é«”è¨˜æ†¶: {e}")
        
        # æ›´æ–°çµ±è¨ˆï¼ˆä½¿ç”¨ merge é¿å…è¦†è“‹ï¼‰
        stats_ref = self.db.collection(self.broadcasts_collection).document(str(current_hour))
        batch.set(stats_ref, {
            'message_count': Increment(1),
            'updated_at': datetime.now(),
            'hour': current_hour
        }, merge=True)
        
        # æ›´æ–°ç”¨æˆ¶è²¢ç»
        if user_id:
            contrib_ref = stats_ref.collection('contributors').document(user_id)
            batch.set(contrib_ref, {
                'count': Increment(1),
                'last_message': datetime.now()
            }, merge=True)
        
        # åŸ·è¡Œæ‰¹æ¬¡å¯«å…¥
        batch.commit()
        
        # ç²å–ä¸¦è¿”å›ç•¶å‰è¨Šæ¯æ•¸
        doc = stats_ref.get()
        message_count = doc.to_dict().get('message_count', 1) if doc.exists else 1
        
        logger.info(f"è¨Šæ¯å·²åŠ å…¥å»£æ’­æ±  - å°æ™‚: {current_hour}, ç¸½æ•¸: {message_count}")
        return message_count
        
    def get_frequency_stats(self):
        """ç²å–ç•¶å‰é »ç‡çµ±è¨ˆ"""
        current_hour = int(time.time()) // 3600
        doc_ref = self.db.collection(self.broadcasts_collection).document(str(current_hour))
        
        # ç²å–çµ±è¨ˆè³‡æ–™
        doc = doc_ref.get()
        if not doc.exists:
            return self._empty_stats()
            
        stats = doc.to_dict()
        message_count = stats.get('message_count', 0)
        
        # ç²å–è²¢ç»è€…æ’è¡Œï¼ˆä½¿ç”¨å¿«ç…§æ¸›å°‘è®€å–æ¬¡æ•¸ï¼‰
        contributors = []
        contrib_query = doc_ref.collection('contributors').order_by('count', direction=firestore.Query.DESCENDING).limit(5)
        
        for contrib_doc in contrib_query.stream():
            data = contrib_doc.to_dict()
            contributors.append((contrib_doc.id, data['count']))
        
        # ç²å–ç¸½ç”¨æˆ¶æ•¸ï¼ˆä½¿ç”¨èšåˆæŸ¥è©¢ï¼‰
        total_users = len(list(doc_ref.collection('contributors').list_documents()))
        
        # è¨ˆç®—é€²åº¦å’Œæ™‚é–“
        progress_percent = min(int((message_count / 1000) * 100), 100)
        messages_needed = max(0, 1000 - message_count)
        
        current_time = int(time.time())
        next_hour = ((current_hour + 1) * 3600)
        time_until_broadcast = next_hour - current_time
        
        # ç°¡å–®çš„è©é »åˆ†æï¼ˆç‚ºäº†ç¯€çœè®€å–æ¬¡æ•¸ï¼Œåªåœ¨è¨Šæ¯å°‘æ–¼100æ™‚åŸ·è¡Œï¼‰
        top_frequencies = []
        if message_count < 100:
            word_freq = {}
            messages_query = doc_ref.collection('messages').limit(100)
            for msg_doc in messages_query.stream():
                content = msg_doc.to_dict().get('content', '')
                words = content.split()
                for word in words:
                    if len(word) > 1:
                        word_freq[word] = word_freq.get(word, 0) + 1
            
            top_frequencies = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]
        
        return {
            'message_count': message_count,
            'progress_percent': progress_percent,
            'messages_needed': messages_needed,
            'time_until_broadcast': {
                'minutes': time_until_broadcast // 60,
                'seconds': time_until_broadcast % 60
            },
            'top_frequencies': top_frequencies,
            'contributors': {
                'total_users': total_users,
                'top_contributors': contributors
            }
        }
    
    def generate_hourly_broadcast(self):
        """ç”Ÿæˆæ¯å°æ™‚çš„é »ç‡å»£æ’­ï¼ˆæ”¯æ´10xå„ªåŒ–ï¼‰"""
        current_hour = int(time.time()) // 3600
        doc_ref = self.db.collection(self.broadcasts_collection).document(str(current_hour))
        
        # æª¢æŸ¥æ˜¯å¦å·²ç”Ÿæˆé
        existing_broadcast = self.db.collection(self.generated_collection).document(str(current_hour)).get()
        if existing_broadcast.exists:
            logger.info(f"å°æ™‚ {current_hour} çš„å»£æ’­å·²å­˜åœ¨")
            return existing_broadcast.to_dict()
        
        # ç²å–æ‰€æœ‰è¨Šæ¯ï¼ˆé™åˆ¶1000å‰‡ï¼‰
        messages = []
        message_dicts = []
        contributors = set()
        messages_query = doc_ref.collection('messages').limit(1000).order_by('timestamp')
        
        for msg_doc in messages_query.stream():
            data = msg_doc.to_dict()
            messages.append(data['content'])
            message_dicts.append({
                'content': data['content'],
                'user_id': data.get('user_id', 'åŒ¿å'),
                'timestamp': data.get('timestamp')
            })
            contributors.add(data.get('user_id', 'åŒ¿å'))
            
        if not messages:
            logger.info(f"å°æ™‚ {current_hour} æ²’æœ‰è¨Šæ¯")
            return None
            
        logger.info(f"æº–å‚™ç”Ÿæˆå»£æ’­ - è¨Šæ¯æ•¸: {len(messages)}")
        
        broadcast_content = ""
        compression_ratio = 1.0
        optimization_type = "standard"
        
        # å˜—è©¦ä½¿ç”¨10xæ ¸å¿ƒåƒ¹å€¼å„ªåŒ–å™¨ï¼ˆElon Muskè¦æ±‚ï¼‰
        if len(messages) >= 100:
            try:
                from optimizations.core_value_optimizer import CoreValueOptimizer
                core_optimizer = CoreValueOptimizer(self.graph)
                optimization_result = core_optimizer.generate_10x_broadcast(message_dicts)
                broadcast_content = optimization_result['broadcast']
                compression_ratio = optimization_result['compression_ratio']
                optimization_type = "10x_optimized"
                logger.info(f"ä½¿ç”¨10xå„ªåŒ–å™¨ï¼Œé”æˆ {compression_ratio:.1f}x å£“ç¸®")
            except Exception as e:
                logger.warning(f"10xå„ªåŒ–å™¨æœªå•Ÿç”¨æˆ–å¤±æ•—: {e}")
        
        # å¦‚æœ10xå¤±æ•—ï¼Œå˜—è©¦é›†é«”è¨˜æ†¶ç³»çµ±
        if not broadcast_content and self.memory_system and len(messages) >= 10:
            try:
                prompt = self.memory_system.generate_broadcast_prompt(current_hour % 24)
                response = self.model.generate_content(prompt)
                if response and response.candidates:
                    broadcast_content = response.candidates[0].content.parts[0].text
                    optimization_type = "collective_memory"
                    logger.info("ä½¿ç”¨é›†é«”è¨˜æ†¶ç³»çµ±ç”Ÿæˆå»£æ’­")
            except Exception as e:
                logger.warning(f"é›†é«”è¨˜æ†¶ç³»çµ±å¤±æ•—: {e}")
        
        # é™ç´šåˆ°æ¨™æº–å»£æ’­
        if not broadcast_content:
            prompt = self._create_prompt(messages)
            try:
                response = self.model.generate_content(prompt)
                if response and response.candidates:
                    broadcast_content = response.candidates[0].content.parts[0].text
                    optimization_type = "standard"
            except Exception as e:
                logger.error(f"Gemini API éŒ¯èª¤: {e}")
                broadcast_content = f"ğŸ“» æœ¬å°æ™‚æ”¶é›†äº† {len(messages)} å‰‡è¨Šæ¯ï¼Œä¾†è‡ª {len(contributors)} ä½æœ‹å‹çš„åˆ†äº«ã€‚"
                optimization_type = "fallback"
        
        # å„²å­˜å»£æ’­çµæœ
        broadcast_data = {
            'content': broadcast_content,
            'timestamp': current_hour * 3600,
            'message_count': len(messages),
            'contributor_count': len(contributors),
            'generated_at': datetime.now(),
            'hour': current_hour,
            'api_calls': 1,
            'optimization_type': optimization_type,
            'compression_ratio': compression_ratio
        }
        
        self.db.collection(self.generated_collection).document(str(current_hour)).set(broadcast_data)
        
        logger.info(f"å»£æ’­ç”ŸæˆæˆåŠŸ - å°æ™‚: {current_hour}, é¡å‹: {optimization_type}")
        
        # è§¸ç™¼æ¸…ç†ä»»å‹™
        self._schedule_cleanup(current_hour)
        
        return broadcast_data
    
    def get_latest_broadcast(self):
        """ç²å–æœ€æ–°çš„å»£æ’­"""
        # æŸ¥è©¢æœ€è¿‘24å°æ™‚çš„å»£æ’­
        twenty_four_hours_ago = int(time.time()) - (24 * 3600)
        
        broadcasts = self.db.collection(self.generated_collection)\
            .where('timestamp', '>=', twenty_four_hours_ago)\
            .order_by('timestamp', direction=firestore.Query.DESCENDING)\
            .limit(1)\
            .stream()
        
        for broadcast in broadcasts:
            return broadcast.to_dict()
            
        return None
    
    def get_broadcast_by_time(self, hour: int):
        """ç²å–ç‰¹å®šæ™‚é–“çš„å»£æ’­"""
        doc = self.db.collection(self.generated_collection).document(str(hour)).get()
        return doc.to_dict() if doc.exists else None
    
    def track_contributor(self, user_id: str):
        """è¿½è¹¤è²¢ç»è€…ï¼ˆå·²æ•´åˆåœ¨ add_to_broadcast ä¸­ï¼‰"""
        pass  # åŠŸèƒ½å·²æ•´åˆ
    
    def get_contributors_stats(self, hour: int):
        """ç²å–è²¢ç»è€…çµ±è¨ˆï¼ˆå·²æ•´åˆåœ¨ get_frequency_stats ä¸­ï¼‰"""
        pass  # åŠŸèƒ½å·²æ•´åˆ
    
    def _empty_stats(self):
        """è¿”å›ç©ºçµ±è¨ˆè³‡æ–™"""
        current_hour = int(time.time()) // 3600
        current_time = int(time.time())
        next_hour = ((current_hour + 1) * 3600)
        time_until_broadcast = next_hour - current_time
        
        return {
            'message_count': 0,
            'progress_percent': 0,
            'messages_needed': 1000,
            'time_until_broadcast': {
                'minutes': time_until_broadcast // 60,
                'seconds': time_until_broadcast % 60
            },
            'top_frequencies': [],
            'contributors': {
                'total_users': 0,
                'top_contributors': []
            }
        }
    
    def _create_prompt(self, messages):
        """å‰µå»º AI æç¤ºè©"""
        # å¦‚æœæœ‰é›†é«”è¨˜æ†¶ç³»çµ±ï¼Œä½¿ç”¨æ™ºæ…§æç¤ºè©
        if self.memory_system:
            current_hour = int(time.time()) // 3600
            return self.memory_system.generate_broadcast_prompt(current_hour)
        
        # å¦å‰‡ä½¿ç”¨åŸæœ¬çš„ç°¡å–®æç¤ºè©
        prompt_prefix = ""
        if len(messages) > 1000:
            messages = messages[:1000]
            prompt_prefix = "ï¼ˆä»¥ä¸‹ç‚ºå‰1000å‰‡è¨Šæ¯ï¼‰\n"
            
        return f"""ä½ æ˜¯ä¸€å€‹é »ç‡å»£æ’­é›»å°çš„ä¸»æŒäººï¼Œå°‡æ”¶é›†åˆ°çš„è¨Šæ¯ç·¨ç¹”æˆå„ªç¾çš„å»£æ’­ã€‚

{prompt_prefix}æ”¶åˆ°çš„è¨Šæ¯ç‰‡æ®µï¼š
{chr(10).join(messages)}

è«‹æ ¹æ“šä»¥ä¸‹è¦æ±‚ç”Ÿæˆå»£æ’­ï¼š
1. æ‰¾å‡ºè¨Šæ¯é–“å…±æŒ¯çš„é »ç‡å’Œæƒ…ç·’ï¼Œä¸è¦å–®ç´”æ‘˜è¦
2. ç”¨æº«æš–æ˜“æ‡‚çš„èªè¨€ï¼Œæ•æ‰é€™å€‹æ™‚åˆ»çš„é›†é«”è„ˆå‹•
3. åæ˜ äººå€‘çš„å…±åŒæƒ…ç·’å’Œé—œæ³¨é»
4. å¦‚æœæŸäº›æƒ…ç·’ç‰¹åˆ¥å¼·çƒˆï¼Œè¦é»å‡ºä¾†
5. é•·åº¦æ§åˆ¶åœ¨150-200å­—
6. ä¸è¦åŠ éŸ³æ¨‚æè¿°æˆ–æ—ç™½æŒ‡ç¤º

è«‹ç”Ÿæˆé€™å€‹å°æ™‚çš„é »ç‡å»£æ’­ï¼š"""
    
    def _schedule_cleanup(self, current_hour):
        """æ’ç¨‹æ¸…ç†ä»»å‹™ï¼ˆå°‡ç”± Cloud Scheduler è™•ç†ï¼‰"""
        # æ¨™è¨˜éœ€è¦æ¸…ç†çš„èˆŠè³‡æ–™
        cleanup_hour = current_hour - 24
        cleanup_ref = self.db.collection('cleanup_tasks').document()
        cleanup_ref.set({
            'hour_to_cleanup': cleanup_hour,
            'scheduled_at': datetime.now(),
            'status': 'pending'
        })
    
    def cleanup_old_data(self):
        """æ¸…ç†è¶…é24å°æ™‚çš„è³‡æ–™"""
        # ç²å–å¾…æ¸…ç†ä»»å‹™
        tasks = self.db.collection('cleanup_tasks')\
            .where('status', '==', 'pending')\
            .limit(10)\
            .stream()
        
        for task_doc in tasks:
            task_data = task_doc.to_dict()
            hour_to_cleanup = task_data['hour_to_cleanup']
            
            try:
                # åˆªé™¤èˆŠçš„å»£æ’­è³‡æ–™
                self._delete_broadcast_data(hour_to_cleanup)
                
                # æ›´æ–°ä»»å‹™ç‹€æ…‹
                task_doc.reference.update({
                    'status': 'completed',
                    'completed_at': datetime.now()
                })
                
                logger.info(f"å·²æ¸…ç†å°æ™‚ {hour_to_cleanup} çš„è³‡æ–™")
                
            except Exception as e:
                logger.error(f"æ¸…ç†å¤±æ•—: {e}")
                task_doc.reference.update({
                    'status': 'failed',
                    'error': str(e)
                })
    
    def _delete_broadcast_data(self, hour: int):
        """åˆªé™¤ç‰¹å®šå°æ™‚çš„å»£æ’­è³‡æ–™"""
        batch = self.db.batch()
        batch_count = 0
        max_batch_size = 500  # Firestore æ‰¹æ¬¡é™åˆ¶
        
        # åˆªé™¤è¨Šæ¯
        doc_ref = self.db.collection(self.broadcasts_collection).document(str(hour))
        messages = doc_ref.collection('messages').list_documents()
        
        for msg_ref in messages:
            batch.delete(msg_ref)
            batch_count += 1
            
            if batch_count >= max_batch_size:
                batch.commit()
                batch = self.db.batch()
                batch_count = 0
        
        # åˆªé™¤è²¢ç»è€…
        contributors = doc_ref.collection('contributors').list_documents()
        for contrib_ref in contributors:
            batch.delete(contrib_ref)
            batch_count += 1
            
            if batch_count >= max_batch_size:
                batch.commit()
                batch = self.db.batch()
                batch_count = 0
        
        # åˆªé™¤ä¸»æ–‡ä»¶
        batch.delete(doc_ref)
        
        # æäº¤æœ€å¾Œçš„æ‰¹æ¬¡
        if batch_count > 0:
            batch.commit()


# ä¿æŒèˆ‡åŸç‰ˆç›¸å®¹çš„è¼”åŠ©å‡½æ•¸
def format_broadcast_message(broadcast_data):
    """æ ¼å¼åŒ–å»£æ’­è¨Šæ¯"""
    if not broadcast_data:
        return "ğŸ“¡ ç›®å‰é‚„æ²’æœ‰å»£æ’­"
        
    timestamp = datetime.fromtimestamp(broadcast_data['timestamp'])
    time_str = timestamp.strftime("%Y-%m-%d %H:00")
    
    message = f"""ğŸŒŠ é »ç‡å»£æ’­
ğŸ“… {time_str}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”

{broadcast_data['content']}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’­ å…± {broadcast_data['message_count']} å€‹è²éŸ³åƒèˆ‡å…±æŒ¯
âœ¨ è¼¸å…¥ã€Œå»£æ’­ã€æŸ¥è©¢æœ€æ–°å…§å®¹"""
    
    return message


def format_stats_message(stats):
    """æ ¼å¼åŒ–çµ±è¨ˆè¨Šæ¯ - å³æ™‚äº’å‹•é¡¯ç¤º"""
    progress_bar = create_progress_bar(stats['progress_percent'])
    
    # å»ºç«‹ç†±è©æ¸…å–®
    hot_words = []
    for word, count in stats['top_frequencies'][:5]:
        hot_words.append(f"ã€Œ{word}ã€Ã—{count}")
    hot_words_str = " ".join(hot_words) if hot_words else "ç­‰å¾…æ›´å¤šè¨Šæ¯..."
    
    # å»ºç«‹è²¢ç»è€…æ’è¡Œ
    contributors_str = ""
    if stats['contributors']['top_contributors']:
        contributors_list = []
        for i, (user, count) in enumerate(stats['contributors']['top_contributors'], 1):
            medal = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else "ğŸ…"
            contributors_list.append(f"{medal} {user} ({count}å‰‡)")
        contributors_str = "\n".join(contributors_list)
    else:
        contributors_str = "æœŸå¾…ç¬¬ä¸€ä½åƒèˆ‡è€…ï¼"
    
    message = f"""ğŸ“Š å³æ™‚é »ç‡çµ±è¨ˆ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â° ä¸‹æ¬¡å»£æ’­å€’æ•¸ï¼š{stats['time_until_broadcast']['minutes']}åˆ†{stats['time_until_broadcast']['seconds']}ç§’

ğŸ“ˆ æœ¬å°æ™‚é€²åº¦
{progress_bar}
ğŸ’¬ {stats['message_count']}/1000 å‰‡ ({stats['progress_percent']}%)

ğŸ”¥ ç†±é–€é »ç‡
{hot_words_str}

ğŸ† åƒèˆ‡æ’è¡Œæ¦œ (å…±{stats['contributors']['total_users']}äºº)
{contributors_str}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¡ ç•¶è¨Šæ¯é”1000å‰‡å°‡æå‰ç”Ÿæˆå»£æ’­ï¼
ğŸ¯ å¿«é‚€è«‹æœ‹å‹ä¸€èµ·åƒèˆ‡å…¨æ°‘å…±ç·¨"""
    
    return message


def format_instant_feedback(message_count, user_rank=None):
    """æ ¼å¼åŒ–å³æ™‚å›é¥‹è¨Šæ¯"""
    milestone_messages = {
        1: "ğŸ‰ ç¬¬ä¸€å‰‡è¨Šæ¯ï¼é–‹å•Ÿé€™å°æ™‚çš„å…±æŒ¯\n\nğŸ’¡ è¼¸å…¥ã€Œç©ã€æ¢ç´¢æ›´å¤šåŠŸèƒ½",
        10: "ğŸŒ± ç¨®å­ç™¼èŠ½äº†ï¼å·²æœ‰10å‰‡è¨Šæ¯\n\nğŸ’¡ è¼¸å…¥ã€Œçµ±è¨ˆã€æŸ¥çœ‹å³æ™‚é€²åº¦",
        50: "ğŸŒ¿ é »ç‡æ¼¸å¼·ï¼50å‰‡è¨Šæ¯é”æˆ",
        100: "ğŸŒ³ ç™¾å‰‡é”æˆï¼è²éŸ³é–‹å§‹å…±é³´\n\nğŸ’¡ è¼¸å…¥ã€Œçœ‹ã€æŸ¥çœ‹å„ç¨®çµ±è¨ˆ",
        250: "ğŸ”¥ å››åˆ†ä¹‹ä¸€é€²åº¦ï¼ç†±åº¦ä¸Šå‡ä¸­",
        500: "âš¡ åŠç¨‹é”æ¨™ï¼500å‰‡è¨Šæ¯",
        750: "ğŸš€ è¡åˆºéšæ®µï¼å‰©æœ€å¾Œ250å‰‡",
        900: "ğŸ’« å³å°‡å®Œæˆï¼é‚„å·®100å‰‡",
        950: "ğŸŠ æœ€å¾Œè¡åˆºï¼é‚„å·®50å‰‡",
        1000: "ğŸ† é”æ¨™ï¼1000å‰‡è¨Šæ¯ï¼Œæº–å‚™ç”Ÿæˆå»£æ’­ï¼"
    }
    
    # å°‹æ‰¾æœ€æ¥è¿‘çš„é‡Œç¨‹ç¢‘
    feedback = ""
    for milestone, msg in milestone_messages.items():
        if message_count == milestone:
            feedback = msg
            break
    
    # å¦‚æœä¸æ˜¯é‡Œç¨‹ç¢‘ï¼Œçµ¦äºˆä¸€èˆ¬å›é¥‹
    if not feedback:
        if message_count < 100:
            feedback = f"âœ¨ ç¬¬{message_count}å‰‡ï¼ç¹¼çºŒåŠ æ²¹"
        elif message_count < 500:
            feedback = f"ğŸŒŠ ç¬¬{message_count}å‰‡ï¼é »ç‡æ¼¸å¼·"
        elif message_count < 900:
            feedback = f"ğŸ”¥ ç¬¬{message_count}å‰‡ï¼å…±æŒ¯å‡æº«"
        else:
            feedback = f"âš¡ ç¬¬{message_count}å‰‡ï¼å³å°‡é”æ¨™"
    
    # åŠ å…¥ç”¨æˆ¶æ’åè³‡è¨Š
    if user_rank and user_rank <= 10:
        rank_emoji = "ğŸ¥‡" if user_rank == 1 else "ğŸ¥ˆ" if user_rank == 2 else "ğŸ¥‰" if user_rank == 3 else "ğŸ…"
        feedback += f"\n{rank_emoji} ä½ æ˜¯ç¬¬{user_rank}åè²¢ç»è€…ï¼"
    
    return feedback


def create_progress_bar(percent):
    """å»ºç«‹è¦–è¦ºåŒ–é€²åº¦æ¢"""
    filled = int(percent / 10)
    empty = 10 - filled
    bar = "â–ˆ" * filled + "â–‘" * empty
    return f"[{bar}]"