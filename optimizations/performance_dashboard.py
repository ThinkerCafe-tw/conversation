"""
æ•ˆèƒ½ç›£æ§å„€è¡¨æ¿ - æä¾›å³æ™‚ç³»çµ±æ•ˆèƒ½æŒ‡æ¨™
"""

import time
try:
    import psutil
except ImportError:
    psutil = None
import os
from typing import Dict, List, Optional
from datetime import datetime, timedelta
from collections import deque
import json

class PerformanceDashboard:
    """ç³»çµ±æ•ˆèƒ½ç›£æ§å„€è¡¨æ¿"""
    
    def __init__(self, redis_client=None, firestore_client=None):
        self.redis = redis_client
        self.firestore = firestore_client
        
        # æ•ˆèƒ½æŒ‡æ¨™å¿«å–
        self.metrics_cache = {
            "api_latency": deque(maxlen=100),      # æœ€è¿‘100æ¬¡APIå‘¼å«å»¶é²
            "db_latency": deque(maxlen=100),       # è³‡æ–™åº«æŸ¥è©¢å»¶é²
            "message_throughput": deque(maxlen=60), # æ¯åˆ†é˜è¨Šæ¯ååé‡
            "concurrent_users": deque(maxlen=60),   # ä¸¦ç™¼ç”¨æˆ¶æ•¸
            "cache_hits": 0,
            "cache_misses": 0
        }
        
        # Gemini API æŒ‡æ¨™
        self.gemini_metrics = {
            "api_calls": 0,
            "total_tokens": 0,
            "avg_response_time": 0
        }
        
        # å„ªåŒ–å»ºè­°
        self.optimization_suggestions = []
        
        # å•Ÿå‹•æ™‚é–“
        self.start_time = time.time()
    
    def record_api_latency(self, endpoint: str, latency_ms: float):
        """è¨˜éŒ„ API å»¶é²"""
        self.metrics_cache["api_latency"].append({
            "endpoint": endpoint,
            "latency": latency_ms,
            "timestamp": time.time()
        })
        
        # å„²å­˜åˆ° Redisï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.redis:
            key = f"metrics:api:{endpoint}:{int(time.time())}"
            self.redis.setex(key, 3600, latency_ms)  # ä¿ç•™1å°æ™‚
    
    def record_db_operation(self, operation: str, duration_ms: float):
        """è¨˜éŒ„è³‡æ–™åº«æ“ä½œ"""
        self.metrics_cache["db_latency"].append({
            "operation": operation,
            "duration": duration_ms,
            "timestamp": time.time()
        })
    
    def record_message(self, user_id: str):
        """è¨˜éŒ„è¨Šæ¯è™•ç†"""
        current_minute = int(time.time() / 60)
        
        # æ›´æ–°ååé‡
        if self.metrics_cache["message_throughput"]:
            last_record = self.metrics_cache["message_throughput"][-1]
            if last_record["minute"] == current_minute:
                last_record["count"] += 1
            else:
                self.metrics_cache["message_throughput"].append({
                    "minute": current_minute,
                    "count": 1
                })
        else:
            self.metrics_cache["message_throughput"].append({
                "minute": current_minute,
                "count": 1
            })
    
    def update_concurrent_users(self, count: int):
        """æ›´æ–°ä¸¦ç™¼ç”¨æˆ¶æ•¸"""
        self.metrics_cache["concurrent_users"].append({
            "count": count,
            "timestamp": time.time()
        })
    
    def update_cache_stats(self, hit: bool):
        """æ›´æ–°å¿«å–çµ±è¨ˆ"""
        if hit:
            self.metrics_cache["cache_hits"] += 1
        else:
            self.metrics_cache["cache_misses"] += 1
    
    def get_system_metrics(self) -> Dict:
        """ç²å–ç³»çµ±è³‡æºä½¿ç”¨æƒ…æ³"""
        if psutil:
            try:
                cpu_percent = psutil.cpu_percent(interval=0.1)
                memory = psutil.virtual_memory()
            except:
                cpu_percent = 0
                memory = type('obj', (object,), {'used': 0, 'total': 1, 'percent': 0})
        else:
            # Cloud Run ç’°å¢ƒå¯èƒ½æ²’æœ‰ psutil
            cpu_percent = 0
            memory = type('obj', (object,), {'used': 0, 'total': 1, 'percent': 0})
        
        return {
            "cpu": {
                "percent": cpu_percent,
                "cores": os.cpu_count() or 1
            },
            "memory": {
                "used_gb": round(memory.used / (1024 ** 3), 2),
                "total_gb": round(memory.total / (1024 ** 3), 2),
                "percent": memory.percent
            },
            "uptime_hours": round((time.time() - self.start_time) / 3600, 2)
        }
    
    def calculate_p95_latency(self) -> Dict[str, float]:
        """è¨ˆç®— P95 å»¶é²"""
        api_latencies = [m["latency"] for m in self.metrics_cache["api_latency"]]
        db_latencies = [m["duration"] for m in self.metrics_cache["db_latency"]]
        
        def get_p95(values):
            if not values:
                return 0
            sorted_values = sorted(values)
            index = int(len(sorted_values) * 0.95)
            return sorted_values[index] if index < len(sorted_values) else sorted_values[-1]
        
        return {
            "api_p95": round(get_p95(api_latencies), 2),
            "db_p95": round(get_p95(db_latencies), 2)
        }
    
    def get_throughput_stats(self) -> Dict:
        """ç²å–ååé‡çµ±è¨ˆ"""
        if not self.metrics_cache["message_throughput"]:
            return {"current": 0, "average": 0, "peak": 0}
        
        counts = [m["count"] for m in self.metrics_cache["message_throughput"]]
        current = counts[-1] if counts else 0
        average = sum(counts) / len(counts) if counts else 0
        peak = max(counts) if counts else 0
        
        # æ›ç®—æˆæ¯ç§’
        return {
            "current_per_sec": round(current / 60, 2),
            "average_per_sec": round(average / 60, 2),
            "peak_per_sec": round(peak / 60, 2)
        }
    
    def get_cache_efficiency(self) -> float:
        """è¨ˆç®—å¿«å–æ•ˆç‡"""
        total = self.metrics_cache["cache_hits"] + self.metrics_cache["cache_misses"]
        if total == 0:
            return 0
        return round(self.metrics_cache["cache_hits"] / total * 100, 2)
    
    def generate_optimization_suggestions(self) -> List[str]:
        """ç”Ÿæˆå„ªåŒ–å»ºè­°"""
        suggestions = []
        
        # åŸºæ–¼å»¶é²
        p95 = self.calculate_p95_latency()
        if p95["api_p95"] > 200:
            suggestions.append("âš¡ API P95 å»¶é²è¶…é 200msï¼Œå»ºè­°å•Ÿç”¨è«‹æ±‚å¿«å–")
        
        # åŸºæ–¼å¿«å–
        cache_efficiency = self.get_cache_efficiency()
        if cache_efficiency < 80:
            suggestions.append(f"ğŸ’¾ å¿«å–å‘½ä¸­ç‡åƒ… {cache_efficiency}%ï¼Œå»ºè­°å„ªåŒ–å¿«å–ç­–ç•¥")
        
        # åŸºæ–¼ Gemini API
        if self.gemini_metrics["avg_response_time"] > 1000:
            suggestions.append("âš¡ Gemini API å›æ‡‰è¼ƒæ…¢ï¼Œè€ƒæ…®ä½¿ç”¨å¿«å–")
        
        # åŸºæ–¼è¨˜æ†¶é«”
        system_metrics = self.get_system_metrics()
        if system_metrics["memory"]["percent"] > 80:
            suggestions.append("ğŸ’¡ è¨˜æ†¶é«”ä½¿ç”¨ç‡é«˜ï¼Œå»ºè­°å¢åŠ è³‡æºæˆ–å„ªåŒ–è¨˜æ†¶é«”ä½¿ç”¨")
        
        # Neo4j å‘é‡ç´¢å¼•å»ºè­°
        if not hasattr(self, "_vector_index_suggested"):
            suggestions.append("ğŸ” å•Ÿç”¨ Neo4j å‘é‡ç´¢å¼•å¯æå‡æŸ¥è©¢é€Ÿåº¦ 40%")
            self._vector_index_suggested = True
        
        return suggestions[:3]  # æœ€å¤šé¡¯ç¤º3å€‹å»ºè­°
    
    def format_dashboard(self) -> str:
        """æ ¼å¼åŒ–å„€è¡¨æ¿è¼¸å‡º"""
        system = self.get_system_metrics()
        p95 = self.calculate_p95_latency()
        throughput = self.get_throughput_stats()
        cache_eff = self.get_cache_efficiency()
        suggestions = self.generate_optimization_suggestions()
        
        # è¨ˆç®—ä¸¦ç™¼ç”¨æˆ¶æ•¸
        concurrent_users = 0
        if self.metrics_cache["concurrent_users"] and len(self.metrics_cache["concurrent_users"]) > 0:
            concurrent_users = self.metrics_cache["concurrent_users"][-1]["count"]
        
        dashboard = f"""ğŸ–¥ï¸ ç³»çµ±ç‹€æ…‹å„€è¡¨æ¿
â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âš¡ æ€§èƒ½æŒ‡æ¨™
â€¢ API å»¶é²: {round(sum(m["latency"] for m in list(self.metrics_cache["api_latency"])[-10:]) / 10, 2) if len(self.metrics_cache["api_latency"]) >= 10 else (round(sum(m["latency"] for m in self.metrics_cache["api_latency"]) / len(self.metrics_cache["api_latency"]), 2) if self.metrics_cache["api_latency"] else 0)}ms (p95: {p95['api_p95']}ms)
â€¢ è¨Šæ¯è™•ç†: {throughput['current_per_sec']} å‰‡/ç§’
â€¢ Gemini API: {self.gemini_metrics['api_calls']} æ¬¡å‘¼å«

ğŸ“Š çŸ¥è­˜åœ–è­œ
â€¢ ç¯€é»ç¸½æ•¸: {self._get_graph_stats().get('nodes', 'N/A')}
â€¢ æŸ¥è©¢ QPS: {throughput['current_per_sec']}
â€¢ å¿«å–å‘½ä¸­ç‡: {cache_eff}%

ğŸ”§ è³‡æºä½¿ç”¨
â€¢ CPU: {system['cpu']['percent']}% ({system['cpu']['cores']} cores)
â€¢ Memory: {system['memory']['used_gb']}GB / {system['memory']['total_gb']}GB
â€¢ Neo4j: {self._estimate_neo4j_memory()}MB

ğŸ’¡ å„ªåŒ–å»ºè­°"""
        
        for suggestion in suggestions:
            dashboard += f"\n{suggestion}"
        
        return dashboard
    
    def format_detailed_metrics(self) -> str:
        """æ ¼å¼åŒ–è©³ç´°æŒ‡æ¨™"""
        recent_api = self.metrics_cache["api_latency"][-5:]
        recent_db = self.metrics_cache["db_latency"][-5:]
        
        details = "ğŸ“ˆ è©³ç´°æ•ˆèƒ½æŒ‡æ¨™\n"
        details += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
        
        if recent_api:
            details += "\næœ€è¿‘ API å‘¼å«:\n"
            for metric in recent_api:
                details += f"â€¢ {metric['endpoint']}: {metric['latency']:.2f}ms\n"
        
        if recent_db:
            details += "\næœ€è¿‘è³‡æ–™åº«æ“ä½œ:\n"
            for metric in recent_db:
                details += f"â€¢ {metric['operation']}: {metric['duration']:.2f}ms\n"
        
        # Gemini API è©³ç´°è³‡è¨Š
        details += f"\nGemini API:\n"
        details += f"â€¢ ç¸½å‘¼å«æ¬¡æ•¸: {self.gemini_metrics['api_calls']}\n"
        details += f"â€¢ ç¸½ Token æ•¸: {self.gemini_metrics['total_tokens']}\n"
        details += f"â€¢ å¹³å‡å›æ‡‰æ™‚é–“: {self.gemini_metrics['avg_response_time']}ms\n"
        
        return details
    
    def _get_graph_stats(self) -> Dict:
        """ç²å–åœ–è³‡æ–™åº«çµ±è¨ˆï¼ˆç°¡åŒ–ç‰ˆï¼‰"""
        # å¯¦éš›æ‡‰è©²å¾ Neo4j æŸ¥è©¢
        return {
            "nodes": "24,531",
            "relationships": "156,789"
        }
    
    def _estimate_neo4j_memory(self) -> int:
        """ä¼°ç®— Neo4j è¨˜æ†¶é«”ä½¿ç”¨"""
        # ç°¡åŒ–ä¼°ç®—
        return 156
    
    def export_metrics(self, format: str = "json") -> str:
        """åŒ¯å‡ºæŒ‡æ¨™è³‡æ–™"""
        data = {
            "timestamp": datetime.now().isoformat(),
            "system": self.get_system_metrics(),
            "performance": {
                "p95_latency": self.calculate_p95_latency(),
                "throughput": self.get_throughput_stats(),
                "cache_efficiency": self.get_cache_efficiency()
            },
            "suggestions": self.generate_optimization_suggestions()
        }
        
        if format == "json":
            return json.dumps(data, indent=2, ensure_ascii=False)
        else:
            # å¯ä»¥æ”¯æ´å…¶ä»–æ ¼å¼å¦‚ CSV, Prometheus ç­‰
            return str(data)


class PerformanceOptimizer:
    """æ•ˆèƒ½å„ªåŒ–å™¨ - è‡ªå‹•èª¿æ•´ç³»çµ±åƒæ•¸"""
    
    def __init__(self, dashboard: PerformanceDashboard):
        self.dashboard = dashboard
        self.optimization_history = []
    
    def auto_optimize(self) -> List[str]:
        """è‡ªå‹•å„ªåŒ–ç³»çµ±åƒæ•¸"""
        optimizations = []
        
        # åŸºæ–¼å»¶é²èª¿æ•´æ‰¹æ¬¡å¤§å°
        p95 = self.dashboard.calculate_p95_latency()
        if p95["api_p95"] > 150:
            optimizations.append("æ¸›å°‘æ‰¹æ¬¡å¤§å°ä»¥é™ä½å»¶é²")
            # å¯¦éš›æ‡‰è©²èª¿æ•´ç³»çµ±åƒæ•¸
        elif p95["api_p95"] < 50:
            optimizations.append("å¢åŠ æ‰¹æ¬¡å¤§å°ä»¥æå‡ååé‡")
        
        # åŸºæ–¼ GPU ä½¿ç”¨ç‡èª¿æ•´ä¸¦è¡Œåº¦
        gpu_util = self.dashboard.gpu_metrics["utilization"]
        if gpu_util < 30:
            optimizations.append("å¢åŠ ä¸¦è¡Œè™•ç†æ•¸ä»¥å……åˆ†åˆ©ç”¨ GPU")
        elif gpu_util > 90:
            optimizations.append("é™ä½ä¸¦è¡Œåº¦ä»¥é¿å… GPU éè¼‰")
        
        # è¨˜éŒ„å„ªåŒ–æ­·å²
        self.optimization_history.append({
            "timestamp": datetime.now(),
            "optimizations": optimizations,
            "metrics": {
                "p95_latency": p95,
                "gpu_utilization": gpu_util
            }
        })
        
        return optimizations


# æ¸¬è©¦ç”¨ä¾‹
if __name__ == "__main__":
    dashboard = PerformanceDashboard()
    
    # æ¨¡æ“¬ä¸€äº›æŒ‡æ¨™
    print("=== æ¨¡æ“¬ç³»çµ±é‹è¡Œ ===\n")
    
    # æ¨¡æ“¬ API å‘¼å«
    for i in range(20):
        endpoint = ["webhook", "stats", "broadcast"][i % 3]
        latency = 50 + (i % 7) * 20  # 50-190ms
        dashboard.record_api_latency(endpoint, latency)
    
    # æ¨¡æ“¬è³‡æ–™åº«æ“ä½œ
    for i in range(15):
        operation = ["query", "insert", "update"][i % 3]
        duration = 10 + (i % 5) * 8  # 10-42ms
        dashboard.record_db_operation(operation, duration)
    
    # æ¨¡æ“¬è¨Šæ¯è™•ç†
    for i in range(100):
        dashboard.record_message(f"user_{i % 10}")
    
    # æ›´æ–°ä¸¦ç™¼ç”¨æˆ¶
    dashboard.update_concurrent_users(847)
    
    # æ¨¡æ“¬å¿«å–
    for i in range(100):
        dashboard.update_cache_stats(i % 10 < 8)  # 80% å‘½ä¸­ç‡
    
    # é¡¯ç¤ºå„€è¡¨æ¿
    print(dashboard.format_dashboard())
    print("\n")
    print(dashboard.format_detailed_metrics())
    
    # æ¸¬è©¦å„ªåŒ–å™¨
    print("\n=== è‡ªå‹•å„ªåŒ–å»ºè­° ===")
    optimizer = PerformanceOptimizer(dashboard)
    suggestions = optimizer.auto_optimize()
    for suggestion in suggestions:
        print(f"â€¢ {suggestion}")
    
    # åŒ¯å‡ºæŒ‡æ¨™
    print("\n=== åŒ¯å‡ºæŒ‡æ¨™ (JSON) ===")
    print(dashboard.export_metrics()[:500] + "...")  # é¡¯ç¤ºå‰500å­—å…ƒ